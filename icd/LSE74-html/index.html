<HTML><HEAD><TITLE>LSE74 Software ICD  - OCS Middleware</TITLE></HEAD>
<BODY BGCOLOR=White>
<IMG SRC="LSST_logo.gif" ALIGN=LEFT>
<H3><BR>LSE-74<BR>
Subsystem : Middle ware</H3>
<HR>
<PRE>
<H3>
</PRE>
1.0 SCOPE<BR>
<UL>
1.1 Document Overview<BR>
1.2 Glossary<BR>
1.3 Applicable Documents<BR>
</UL>
2.0 CONCEPT OF OPERATIONS<BR>
<UL>
2.1 System Overviews<BR>
2.2 Service abstraction layer<BR>
<UL>
2.2.1 Interfaces - C/C++<BR>
2.2.2 Interfaces - Java<BR>
2.2.3 Interfaces - Tcl<BR>
2.2.4 Interfaces - Labview<BR>
2.2.5 Interfaces - Python<BR>
</UL>
2.3 SAL Tools<BR>
<UL>
2.3.1 Salgenerator<BR>
2.3.2 Virtual machine<BR>
</UL>
2.4 OMG DDS<BR>
<UL>
2.4.1 Interfaces - C/C++<BR>
2.4.2 Interfaces - Java<BR>
</UL>
2.5 DDS Tools<BR>
<UL>
2.5.1 Data definition<BR>
2.5.2 Code generation<BR>
2.5.3 Debug<BR>
</UL>
2.6 OMG RTPS wire protocol<BR>
2.7 Security and Integrity<BR>
<UL>
2.7.1 General policies<BR>
2.7.2 Firewall<BR>
2.7.3 Packet filtering<BR>
2.7.4 Private subnet<BR>
2.7.5 DDS domains<BR>
</UL></UL>
3.0 DETAILED INTERFACE REQUIREMENTS<BR>
<UL>
3.1 Commanding Requirements<BR>
<UL>
3.1.1 Generic subsystem control state commands<BR>
3.1.2 Interface Processing Time Requirements<BR>
3.1.3 Message Requirements<BR>
</UL>
3.2 Telemetry Requirements<BR>
3.3 Event Notifications Requirements<BR>
3.4 Communication Methods<BR>
<UL>
3.4.1 Initiation : DDS discovery<BR>
3.4.2 Flow Control : DDS topics<BR>
</UL>
3.5 Security Requirements<BR>
<UL>
3.5.1 Message timestamps<BR>
3.5.2 Software versioning checksums<BR>
</UL></UL>
4.0 QUALIFICATION METHODS<BR>
<UL>
4.1 System dictionary<BR>
4.2 Command definition database<BR>
4.3 Telemetry datastream Definition database<BR>
4.4 Code generation<BR>
4.5 Testing<BR>
</UL>
5   Appendicies :<BR>
<UL>
A    System Dictionary<BR>
B1   Interface Definitions - Auxiliary Telescope<BR>
B2   Interface Definitions - Calibration<BR>
B3   Interface Definitions - Camera<BR>
B4   Interface Definitions - Data Management<BR>
B5   Interface Definitions - Dome<BR>
B6   Interface Definitions - Environment<BR>
B7   Interface Definitions - Laser Calibration<BR>
B8   Interface Definitions - M1M3<BR>
B9   Interface Definitions - M2<BR>
B10  Interface Definitions - Mount<BR>
B10b Interface Definitions - Mount.hexapod<BR>
B10c Interface Definitions - Mount.rotator<BR>
B11  Interface Definitions - Network<BR>
B12  Interface Definitions - OCS Operations<BR>
B13  Interface Definitions - Power<BR>
B14  Interface Definitions - Scheduler<BR>
B15  Interface Definitions - Seeing<BR>
B16  Interface Definitions - Sky Cameras<BR>
B17  Interface Definitions - System<BR>
B18  Interface Definitions - Telescope Control<BR>
C    Engineering and Facility Database<BR>
</UL>
<HR>
<P>
<H2>1.0 SCOPE</H2>
This Interface Control Document (ICD) specifies the interface(s) between OCS Middleware and other subsystems. Upon formal approval by the line Manager responsible for each participating system, this ICD shall be incorporated into the requirements baseline for each system.
<P>

<H2>1.1       Document Overview</H2>

This document describes the middleware (software stack)
used for subsystem communications. The publish-subscribe
architecture is used as the basis. Open standards (OMG Data Distribution Service)
describe the low level protocols and API's. The OpenSplice DDS
implementation is being used to provide this framework.
<P>
An additional  "Service Abstraction Layer" will provide application developers
with a simplified interface to the DDS facilities. It also facilitates
the logging of all subsystem telemetry and command history.
to the Engineering and Facility Database (EFD).
<P>
Tools are used to automatically generate communications code
from Telemetry and Command definitions, which are
described using "Interface definition Language".
<P>
A System Dictionary describes the syntax and naming schemes
used in the IDL, thus establishing system-wide consistency.
<P>
The required network bandwidth for each subsystem , and the
accompanying EFD table sizes are described.
<P>
The detailed descriptions of all the Telemetry streams and
commands, are listed in per-subsystem appendices.
<P>
<H2>1.2 Glossary</H2>
<P><BR>BEE - Back-End Electronic<BR>
CALSYS - Camera Calibration System<BR>
CCS - Camera Control System<BR>
DDS - Data Distribution Service<BR>
EA - Enterprise Architect<BR>
EFD - Engineering and Facility Database<BR>
FCS - Filter Controller Subsystem<BR>
FEE - Front End Electronics<BR>
FPA - Focal Plane Array Actuation<BR>
GAS - Guider data acquisition system<BR>
GS - Guider system<BR>
IDL - Interface Definition Language<BR>
L2U - L2 Controller Unit<BR>
LASERCAL - Camera metrology calibration System<BR>
LSST - Large Synoptic Survey Telescope<BR>
ODBC - Open Database Connectivity<BR>
OMG - Object Management Group<BR>
PWR - Camera Power supply System<BR>
QA - Camera Quality Assurance measurements<BR>
QoS - Quality of Service<BR>
RAS - Raft Alignment Subsystem<BR>
RTI - Real Time Innovation<BR>
RTPS - Real Time Publish Subscribe<BR>
SAL - Service Abstraction Layer<BR>
SAS - Science Array System<BR>
sbt - Simple Build Tool<BR>
SCU - Shutter Controller Unit<BR>
SDS - Science Data Acquisition System<BR>
SQL - Standard Query Language<BR>
TC - Thermal Control<BR>
TCM - Camera Timing control<BR>
unixODBC - Open Database Connectivity for unix<BR>
VCS - Vacuum Control Subsystem<BR>
WDS - Wavefront data acquisition system<BR>
WFS - Wave-front Sensing System<BR>
WTCM - Camera wavefront sensors Timing control<BR>
XML - eXtensible Markup Language<BR>


<P>
<H2>1.3 Applicable Documents</H2><P>
<BR>Datastream Definitions Document - Datastream Prototypes 1.7 (Document-11528)
<BR>Definition of subsystems - LSST Project WBS Dictionary (Document-985)
<BR>Documentation standards - LSST DM UML Modeling Conventions (Document-469)
<BR>Messaging standards - OMG DDS 1.1 (Document-2233)
<BR>Security policies - http://dev.lsstcorp.org/trac/attachment/wiki/Security/Security Policy documents.zip
<BR>Software Abstraction Layer API  - Middleware Software Abstraction Layer (Document-3692)
<BR>Software coding standards LSST C++ Programming Style Guidelines (Document-3046)
<BR>Vendor documentation - Opensplice manuals (Collection-2791)
<P>

<H2>2.0 CONCEPT OF OPERATIONS</H2>
<H2>2.1 System Overview</H2>
<BR>
The publish-subscribe communications model provides a more efficient model for broad data distribution over a network than point-to-point, client-server, and distributed object models. Rather than each node directly addressing other nodes to exchange data, publish-subscribe provides a communications layer that delivers data transparently from the nodes publishing the data to the nodes subscribing to the data.
Publishers send events to the communications layer, which in turn, passes the events to subscribers. In this way, a single event published by a publisher can be sent to multiple subscribers. Events are categorized by topic, and subscribers specify the topics they are interested in. Only events that match a subscribers topic are sent to that subscriber. The service permits selection of a number of quality-of-service criteria to allow applications to choose the appropriate trade-off between reliability and performance.
<P>
The combination of Client-Server and Publish-Subscribe models leads to the concept of Command/Action/Response model, in that the transmission of commands is decoupled from the action that executes that command. A command will return immediately; the action begins in a separate thread. Figure 3 illustrate this model by means of a simplified sequence diagram. When an application receives a command, it validates the attributes associated with that command and immediately accepts or rejects the command. If the command is accepted, the application then initiates an independent internal action to meet the conditions imposed by the command. Once those conditions have been met, an event is posted signifying the successful completion of the action (or the unsuccessful completion if the condition not be met). In this figure, callbacks are implemented using the event features of the publish-subscribe model.
<P>
<IMG SRC="omg.jpg"><P>
Information flows with the aid of the following constructs : Publisher and DataWriter
on the sending side, Subscriber, and DataReader on the receiving side.
<P>
A Publisher is an object responsible for data distribution. It may publish data of
different data types. A DataWriter acts as a typed4 accessor to a publisher. The
DataWriter is the object the application must use to communicate to a publisher the
existence and value of data-objects of a given type. When data-object values have
been communicated to the publisher through the appropriate data-writer, it is the
publisher's responsibility to perform the distribution (the publisher will do this
according to its own QoS, or the QoS attached to the corresponding data-writer). A
publication is defined by the association of a data-writer to a publisher. This
association expresses the intent of the application to publish the data described by
the data-writer in the context provided by the publisher.
<P>
A Subscriber is an object responsible for receiving published data and making it
available (according to the Subscribers QoS) to the receiving application. It may
receive and dispatch data of different specified types. To access the received data,
the application must use a typed DataReader attached to the subscriber. Thus, a
subscription is defined by the association of a data-reader with a subscriber. This
association expresses the intent of the application to subscribe to the data described
by the data-reader in the context provided by the subscriber.
<P>
Topic objects conceptually fit between publications and subscriptions. Publications must
be known in such a way that subscriptions can refer to them unambiguously. A Topic is
meant to fulfill that purpose: it associates a name (unique in the domain), a data-type,
and QoS related to the data itself. In addition to the topic QoS, the QoS of the
DataWriter associated with that Topic and the QoS of the Publisher associated to the
DataWriter control the behavior on the publisher's side, while the corresponding Topic,
DataReader, and Subscriber QoS control the behavior on the subscribers side.
<P>
When an application wishes to publish data of a given type, it must create a Publisher
(or reuse an already created one) and a DataWriter with all the characteristics of the
desired publication. Similarly, when an application wishes to receive data, it must create
a Subscriber (or reuse an already created one) and a DataReader to define the
subscription.
<P>
QoS (Quality of Service) is a general concept that is used to specify the behavior of a
service. Programming service behavior by means of QoS settings offers the advantage
that the application developer only indicates what is wanted rather than how this QoS
should be achieved. Generally speaking, QoS is comprised of several QoS policies. Each
QoS policy is then an independent description that associates a name with a value.
Describing QoS by means of a list of independent QoS policies gives rise to more
flexibility.
<P>
This specification is designed to allow a clear separation between the publish and the
subscribe sides, so that an application process that only participates as a publisher can
embed just what strictly relates to publication. Similarly, an application process that
participates only as a subscriber can embed only what strictly relates to subscription.
<P>
Underlying any data-centric publish subscribe system is a data model. This model
defines the global data space and specifies how Publishers and Subscribers refer to
portions of this space. The data-model can be as simple as a set of unrelated data structures,
each identified by a topic and a type. The topic provides an identifier that
uniquely identifies some data items within the global data space. The type provides
structural information needed to tell the middleware how to manipulate the data and also
allows the middleware to provide a level of type safety. However, the target applications
often require a higher-level data model that allows expression of aggregation and
coherence relationships among data elements.
<P>

The OCS Middleware provides multiple levels of access to the functionality
provided. It is recommended that the highest level methods be utilized
whenever possible. <P>
The access levels are : 
<UL>
<LI>SAL (Software abstraction layer)
<LI>SIMD/jSIMD (Simplified DDS)
<LI>OMG DDS (Data Distribution Service)
<LI>OMG RTPS (Real-time Publish Subscribe) - not used directly at present
</UL>

Each subsequent layer provides more detailed access to the low-level details
of the configuration and control of the datastream definitions and/or tuning of
real-time behavior.



<P>
<BR>
<H3>2.2	SAL (Service abstraction layer)</H3>

The SAL provides the highest level of access to the Middleware functionality.<P>
Transparent access to telemetry and command objects residing on any<BR>
subsystem is provided via means of automatic shared memory mapping of the <BR>
underlying data objects.
<P> The lower level objects are managed using an implementation<BR>
of the OMG's DDS. 
<P>The currently selected implementation is OpenSplice DDS, but the existence
of the SAL permits flexibility in migrating to other DDS solutions if required.
<P>
The SAL provides direct access to only a small subset of the total functionality<BR>
provided by the DDS, reducing both the amount of code required, and it's<BR>
complexity, as seen by the application programmer.
<P>
The OMG DDS standard is an evolving entity. It is expected that the prototype SIMD/jSIMD
API's referenced below, will be replaced by agreed OMG standards of equivalent
functionality. 
<P>
The SAL framework is designed to make this, and other similar
transitions, transparent to the application level developers.
<P>
<P><HR><HR><P>
<H4>2.2.1 Interfaces - C/C++</H4>
The SAL C++ interface provides per-subsystem and per-datastreams specific objects
to facilitate application level publishing of all telemetry.
<P>
See the <A HREF="sal/class_s_a_l__hexapod.html">Example generated classes</A> for details.
<P>
The SAL will provide per-subsystem and per-datastreams specific objects
to facilitate application level access to all published telemetry.
<P>
The SAL will provide automatic version and temporal consistency
checking and appropriate feedback to the application level code.
<P>
<H4>2.2.2 Interfaces - Java</H4>
The SAL Java interface will provide per-subsystem and per-datastreams specific objects
to facilitate application level publishing of all telemetry.
<P>
See the <A HREF="sal/classorg_1_1lsst_1_1sal_1_1_s_a_l__hexapod.html">Example generated classes</A> for details.
<P>
The SAL will provide per-subsystem and per-datastreams specific objects
to facilitate application level access to all published telemetry.
<P>
The SAL will provide automatic version and temporal consistency
checking and appropriate feedback to the application level code.
<P>
<P>
<P><P><HR><P>
<P><HR><P>
<H4>2.2.3 Interfaces - Tcl</H4>

The tcl scripting interface will provide a quick and easy to use<BR>
testing harness for experimenting with the SAL based <BR>
datastreams and command/response topics.
<P>
The interface is initialized by loading a shared library into<BR>
a standard "wish" command shell.
<P>
The following command are to be available for scripting<BR>
or interactive use
<P><P><P><PRE>
readshm <EM>--TOPICID--</EM>
</PRE><P>
Reads the current values for the topic from the shared<BR>
memory area. Tcl variables with names of the form<BR>
SHM<EM>--TOPICID--</EM>(itename) are populated wit the values.
<P><P><P><PRE>
writeshm <EM>--TOPICID--</EM>
</PRE><P>
Publishes the current values for the topic from Tcl variables<BR>
to the appropriate shared memory area.
<P><P><P><PRE>
sendcmd <EM>--SUBSYSTEM--</EM> args...
</PRE><P>
Send a command to the named subsystem. 
Responses can be received by using the command
readshm --SUBSSYTEM--_response
<P><P><P><PRE>
sendack <EM>--SUBSYSTEM--</EM> args...
</PRE><P>
Send a command response for the named subsystem.
<P><P><P><PRE>
logevent <EM>--SUBSYSTEM--</EM> args...
</PRE><P>
Log an alert/warning/message for the named subsystem.
<P>


<H4>2.2.4 Interfaces - Labview</H4>
The SAL Labview interface will provide per-subsystem and per-datastreams specific objects
to facilitate application level publishing of all telemetry.
<P>
The SAL will provide automatic version and temporal consistency
checking and appropriate feedback to the application level code.
<P>

<H4>2.2.5 Interfaces - Python</H4>
The SAL will provides a Boost.Python interface which wraps the C++/Java API. 
See the SAL SDK User Guide for details.
<P><HR><P>
<P><HR><P>

<H4>2.3 SAL Tools</H4>

A combination of methods are provided to facilitate data definition, 
command definition, and associated generation of code and documentation.

<H4>2.3.1 Salgenerator</H4>
The Salgenerator tool and associated SDK provide a simple (command
line interface) method of interacting with all the tools included
ih the SAL.<P>
Invocation with no arguments will result in display of the on-line help.
<P>
<PRE>
SAL generator tool - Usage :

	salgenerator match*.idl flag(s)

   where flag(s) may be

		validate - check the IDL and Command/LogEvent definitions
		simd     - generate SIMD c++ code
                sal      - generate SAL wrappers for cpp, java, isocpp, python
		compile  - compile a c++ module
		java     - generate JNI interface
		tcl      - generate tcl interface
		html     - generate web form interfaces
		labview  - generate Labview low-level interface
		db       - generate telemetry database table

                    Arguments required are
 
		    db start-time end-time interval

                    where the times are formatted like "2008-11-12 16:20:01"
                    and the interval is in seconds

		shmem	 - generate shared memory interface
		sim      - generate simulation configuration
		icd      - generate ICD document
		link     - link a SAL program
		verbose  - be more verbose ;-)

</PRE>

<H4>2.3.2 Virtual machine</H4>
The SAL data definition tasks may also be instantiated in a virtual<BR>
machine toolset. The VM can be loaded into a VMware or<BR>
VirtualBox session. Once the VM is running, it will provide<BR>
a browser based interface. <BR>
<P>
The initial page look like this
<P>
<IMG SRC="vm-reg.gif"><P>

Once the user has registered the VM instance, then data<BR>
definition tasks can be started. review the instructions<BR>
<P>
<IMG SRC="vm-datadef.gif"><P>
then click on the "Datastream Definition Editor".<BR>

The editor page is prepopulated with the default set of <BR>
datastream definitions. For each datastream there are<BR>
3 choices of activity.<BR>
<P>
<IMG SRC="vm-datadef2.gif"><P>
To edit the definition, choose the "Edit contents" option.<BR>
For example, selecting tcs_kernel_Target would open to a page like this<BR>
<P>
<IMG SRC="vm-datadef3.gif"><P>

Here , items can be added, modified, or deleted. Remember to click<BR>
update (bottom of the page) once all required changes have been <BR>
made.<P>
Once the data definitions have been created the SAL VM will <BR>
generate the corresponding IDL file for input into the<BR>
code generation tools.
<P>
In the event that a pre-existing IDL file has been generated<BR>
then this can also be passed to the SAL VM and syntax checked<BR>
prior to use.
<P>
<A HREF="sampleidl.txt">Sample IDL file defining default Camera datastreams.</A>
 <P>
<P><HR><P>
<H4>Code generation</H4>

The SAL VM includes the capability to generate implementation<BR>
code for a range of languages. Currently supported are<BR>
<P>
<UL>
<LI>C/C++ GNU compilers
<LI>Java
<LI>Python
<LI>Tcl
</UL>

The code generated will provide interfaces to the following<BR>
packages and environments<P>
<UL>
<LI>OpenSplice DDS
<LI>Labview
<LI>SQL databases
<LI>EA DDS XML
</UL>

The code generation is selected via the "setup" option <BR>
on the Datastream definition editor page.<P>
For example, selecting the tcs.kernel.Target "setup" option<BR>
leads to a page like this
<P>
<IMG SRC="vm-codegen1.gif"><P>
<P>
<IMG SRC="vm-codegen2.gif"><P>

In this example , the user has selected to generate code for<BR>
the tcs.kernel.FK5Target item, opting to both publish telemetry<BR>
and process incoming commands.<P>

The generated code will be C/C++ and include support for the <BR>
OpenSplice DDS low-level software. 
<P>
The code generation process logs it's progress to the browser window,<BR>
and upon successful completion it builds archives containing the <BR>
generated code, and presents a set of links for downloading them.
<P>
<IMG SRC="vm-codegen3.gif"><P>

The SAL SDK User Guide contains a comprehensive of the generated
files and their contents.

<P>
<P><HR><P>

<H4>Simulation</H4>

The SAL VM includes the capability to generate subsystem test<BR>
deployable to multiple machines. It uses scp to copy data and <BR>
programs to the participating machines, so they must be setup with<BR>
the appropriate ssh environment and public-key infrastructure.
<P>
Simulation setup is performed using a form where each datastream<BR>
or commanding participant in a subsystem can be allocated to <BR>
a different computer.<P>
<P>
<IMG SRC="vm-sim1.gif">
<P>
Once the simulation definition has been completed, the VM builds
all the appropriate libraries and test executables. A simulation<BR>
deployment script is created. This script is designed to be run <BR>
from the SAL VM, either by logging in to an ssh terminal session, <BR>
or by connecting to the built-in VNC desktop. For example
<P>
<IMG SRC="vm-sim2.gif">
<P>
shows the VNC desktop with an xterm listing the launcher script<BR>
for a tcs subsystem simulation.
<P><HR><P>
The simulation capabilities also include database creation.<BR>
For each of the defined datastreams and command/response<BR>
streams, the user can select a start and end epoch for the <BR>
simulation, and whether real-time , or a dump to file is<BR>
preferred.<P>
The following example would create an SQL dump file for the<BR>
tcs_kernel_FK5Target datastream, for a 1 hour period.
<P>
<IMG SRC="vm-sim4.gif"><P>
The resulting dump is subsequently presented for download<P>
<IMG SRC="vm-sim5.gif">
<P>
<P><HR><P>

<P><HR><P>

<H3>2.4	OMG DDS</H3>
The OMG Data-Distribution Service (DDS) is a specification for publish-subscribe data-distribution systems. The purpose of the specification is to provide a common application-level interface that clearly defines the data-distribution service. The specification describes the service using UML, providing a platform-independent model that can then be mapped into a variety of concrete platforms and programming languages.
<P>
The goal of the DDS specification is to facilitate the efficient distribution of data in a distributed system. Participants using DDS can read and write data efficiently and naturally with a typed interface. Underneath, the DDS middleware will distribute the data so that each reading participant can access the most-current values. In effect, the service creates a global data space that any participant can read and write. It also creates a name space to allow participants to find and share objects.
<P>
DDS targets real-time systems; the API and QoS are chosen to balance predictable behavior and implementation efficiency/performance.
<P>
<P><HR><P>

<H4>2.4.1 Interfaces - C/C++</H4>
See the <A HREF="dds-c++/annotated.html">OpenSplice DDS C++ API documentation</A> for details.
<P>
<H4>2.4.2 Interfaces - Java</H4>
See the <A HREF="dds-java/annotated.html">OpenSplice DDS Java API documentation</A> for details.
<P>
<H4>2.5 DDS Tools</H4>
<H4>2.5.1 Code generation</H4>
The DDS standard provides an source code generation tool, the IDL Pre-Processor (idlpp)
which can generate DSS interface code for a variety of language/environment combinations.
We use the "standalone C++", and "standalone Java" variants.
<P>
<H4>Opensplice Gateway</H4>
The OpenSplice Gateway provides semi-automated message translation between a 
large number of middleware systems.
<P>
By leveraging the Apache Camel integration framework and its support for over 80 connectors, the OpenSplice Gateway is the best choice for integrating DDS-interoperable applications with proprietary as well as standards-based messaging technologies, such as JMS and AMQP, as well as user applications leveraging Web standards such as W3C Web Services, REST and HTML5 WebSockets.<P>
 It's potential use is still being evaluated.

<P>
<H4>2.5.2 Debug</H4>
<H4>Opensplice Tuner</H4>
The OpenSplice Tuner is a deployment tool within PrismTech's OpenSplice DDS suite. This tool offers total control over a deployed OpenSplice based DDS-system from any local or remote platform that supports the Java language.
<P>
The Java based OpenSplice Tuner tool aids the design, implementation, test and maintenance of OpenSplice based distributed systems (the OpenSplice Tuner is available both as a 'standalone' Java-program as well as an Eclipse plug-in for the Productivity tool suite).
<P>
The OpenSplice Tuner's features target all lifecycle stages of distributed system development and can be summarized as:
<P><UL>
<LI>    Design: During the design phase, once the information model is established (i.e. topics are defined and 'registered' in a runtime environment, which can be both a host-environment as well as a target-environment), the Tuner allows creation of publishers/writers and subscribers/readers on the fly to experiment and validate how this data should be treated by the middleware regarding persistence, durability, latency, etc.
<LI>    Implementation: During the implementation phase, where actual application-level processing and distribution of this information is developed, the OpenSplice Tuner allows injection of test input-data by creating publishers and writers 'on the fly' as well as validating the responses by creating subscribers and readers for any produced topics.
<LI>    Test: during the test phase, the total system can be monitored by inspection of data (by making 'snapshots' of writer- and reader-history caches) and behavior of readers & writers (statistics, like how long data has resided in the reader's cache before it was read) as well as monitoring of the data-distribution behavior (memory-usage, transport-latencies).
<LI>    Maintenance: Maximum flexibility for planned and 'ad-hoc' maintenance is offered by allowing the Tuner tool (which can be executed on any JAVA enabled platform without the need of OpenSplice to be installed) to remotely connect via the web-based SOAP protocol to any 'reachable' OpenSplice system around the world (as long a HTTP-connection can be established with the OpenSplice computing-nodes of that system).  Using such a dynamic-connection, critical data may be logged and data-sets may be 'injected' into the system to be maintained (such as new settings which can be automatically 'persisted' using the QoS features as offered by the 'persistence-profile supported by OpenSplice).
</UL><P>

<H4>Opensplice Tester</H4>
This Java based tool is designed with the systems integrator in mind and offers an intuitive set of features to aid his task, offering both local operation (where the tool is running on a deployed DDS-system) as well as remote operation (where the tool is connect over SOAP to a remotely deployed DDS-system).
<P>
The main features of the OpenSplice Tester are:
<P><UL>
<LI>    Automated testing of DDS-based systems
<UL>
<LI>        Dynamic discovery of DDS entities
<LI>        Domain-Specific scripting Language (DSL) for test scenario's
</UL>
<LI>        Batch execution of regression tests
<UL>
<LI>    Debugging of distributed DDS system
<LI>        One-click definition of a monitoring-time-line
<LI>        Analysis/comparison of topics/instances & samples
<LI>        Virtual topic-attributes to dramatically ease analysis
<LI>        System-browser of DDS entities (app's/readers/writers)
<LI>        Connectivity and QoS-conflict monitoring/detection
<LI>        Statistics-monitoring of applications and services
</UL>
<LI>    Integrated IDE
<UL>
<LI>        Syntax highlighting editor, script-executor and Sample Logger
<LI>        One-click relations between script, logs and timeline
<LI>        Optional integration of message-interfaces with DDS interactions
</UL></UL>



<H4>DDS touchstone</H4>
<A HREF="http://dds-touchstone.sourceforge.net/">
DDS Touchstone</A> is a scenario-driven Open Source benchmarking framework for evaluating the performance of OMG DDS compliant implementations. It is an open-source project.
<P>
Its main characteristics are:
<UL>
<LI>    A deployment, language, OS en vendor neutral benchmarking suite
<LI>    Measures and outputs roundtrip latencies and throughput numbers
<LI>    Supports creation of many different kinds of DDS scenarios
<LI>    Supports recording and replaying scenarios
</UL>
<P>

<H4>DDS Benchmark Environment</H4>
DBE is the DDS Benchmarking Environment is used to test the DDS implementation. The DBE consists of scripts to automate test running on a distributed network. These scripts are written in Perl and use NFS partitions to synchronize and log results. The DBE is the kernel of what may later become a unified testing architecture for any type of middleware.<P>
This DBE is freely available from the 
<A HREF="http://www.dre.vanderbilt.edu/DDS/html/overview.html">
Real-Time DDS Examination & Evaluation Project (RT-DEEP)</A><P>

<H3>2.6	OMG RTPS wire protocol</H3>
<B>The RTPS layer is NOT expected to be used directly by any project generated code, 
we included a brief description for completeness.</B><P>

The Real-Time Publish Subscribe (RTPS) protocol has its roots in industrial automation and was approved by the IEC as part of the Real-Time Industrial Ethernet Suite IEC-PAS-62030. It is a field proven technology that is currently deployed worldwide in thousands of industrial devices.
RTPS was specifically developed to support the unique requirements of data-distributions systems. 
<P>
As one of the application domains targeted by DDS, the industrial automation community defined requirements for a standard publish subscribe wire-protocol that closely match those of DDS. 
There is a close synergy between DDS and the RTPS wire-protocol, both in terms of the underlying behavioral architecture and the features of RTPS.
<P>
The RTPS protocol is designed to be able to run over multicast and connectionless best-effort transports such as UDP/IP. The main features of the RTPS protocol include:
<P>
<UL>
<LI>Performance and quality-of-service properties to enable best-effort and reliable publish-subscribe communications for real-time applications over standard IP networks.
<LI>Fault tolerance to allow the creation of networks without single points of failure.
<LI>Extensibility to allow the protocol to be extended and enhanced with new services without breaking backwards compatibility and interoperability.
<LI>Plug-and-play connectivity so that new applications and services are automatically discovered and applications can join and leave the network at any time without the need for reconfiguration.
<LI>Reconfigurability to allow balancing the requirements for reliability and timeliness for each data delivery.
<LI>Modularity to allow simple devices to implement a subset of the protocol and still participate in the network.
<LI>Scalability to enable systems to potentially scale to very large networks.
<LI>Type-safety to prevent application programming errors from compromising the operation of remote nodes.
</UL>
<P>
The above features make RTPS an excellent match for a DDS wire-protocol. Given its publish
subscribe roots, this is not a coincidence, as RTPS was specifically designed for meeting the types of requirements set forth by the DDS application domain.
<P>
This specification defines the message formats, interpretation, and usage scenarios that underlie all messages exchanged by applications that use the RTPS protocol.
<P>
See the <A HREF="dds_rtps.pdf">RTPS Specification Document</A> for details.
<P>
<H2>2.7 Security and Integrity</H2>
<BR>
<H4>2.7.1 General policies</H4>

Refer to  http://dev.lsstcorp.org/trac/attachment/wiki/Security/Security Policy documents.zip<P>

<H4>2.7.2 Firewall</H4>
A firewall's basic task is to regulate the flow of traffic between computer networks of different trust levels. Typical examples are the Internet which is a zone with no trust and an internal network which is a zone of higher trust. A zone with an intermediate trust level, situated between the Internet and a trusted internal network, is often referred to as a perimeter network or Demilitarized zone (DMZ).

<H4>2.7.3 Packet filtering</H4>
Packet filters act by inspecting the packets which represent the basic unit of data transfer between computers on the Internet. If a packet matches the packet filter's set of rules, the packet filter will drop (silently discard) the packet, or reject it (discard it, and send error responses to the source).
<P>
This type of packet filtering pays no attention to whether a packet is part of an existing stream of traffic (it stores no information on connection state). Instead, it filters each packet based only on information contained in the packet itself (most commonly using a combination of the packet's source and destination address, its protocol, and, for TCP and UDP traffic, which comprises most internet communication, the port number).
<P>
Because TCP and UDP traffic by convention uses well known ports for particular types of traffic, a stateless packet filter can distinguish between, and thus control, those types of traffic (such as web browsing, remote printing, email transmission, file transfer), unless the machines on each side of the packet filter are both using the same non-standard ports.
Second Generation firewalls do not simply examine the contents of each packet on an individual basis without regard to their placement within the packet series as their predecessors had done, rather they compare some key parts of the trusted database packets. This technology is generally referred to as a 'stateful firewall' as it maintains records of all connections passing through the firewall, and is able to determine whether a packet is the start of a new connection, or part of an existing connection. Though there is still a set of static rules in such a firewall, the state of a connection can in itself be one of the criteria which trigger specific rules.
<P>
This type of firewall can help prevent attacks which exploit existing connections, or certain Denial-of-service attacks, including the SYN flood which sends improper sequences of packets to consume resources on systems behind a firewall.
<P>
<H4>2.7.4 Private subnet</H4>
Firewalls often have network address translation (NAT) functionality, and the hosts protected behind a firewall commonly have addresses in the private address range, as defined in RFC 1918. Firewalls often have such functionality to hide the true address of protected hosts. Originally, the NAT function was developed to address the limited amount of IPv4 routable addresses that could be used or assigned to companies or individuals as well as reduce both the amount and therefore cost of obtaining enough public addresses for every computer in an organization. Hiding the addresses of protected devices has become an increasingly important defense against network reconnaissance.


<H4>2.7.5 DDS domains</H4>
The domain is the basic construct used to bind
individual applications together for
communication. A distributed application can
elect to use a single domain for all its data-centric
communications. 
<P>
All Data Writers and Data Readers with like data
types will communicate within this domain.
DDS also has the capability to support multiple
domains, thus providing developers a system that
can scale with system needs or segregate based
on different data types. When a specific data
instance is published on one domain, it will not
be received by subscribers residing on any other
domains.
<P>
Multiple domains provide effective data
isolation. One use case would be for a system to
be designed whereby all Command/Control
related data is exchanged via one domain while
Status information is exchanged within another. 
Multiple domains are also a good way to control
the introduction of new functionality into an
existing system.
<P>



<H2>3.0 DETAILED INTERFACE REQUIREMENTS</H2>
<H2> 3.1 Commanding Requirements</H2>
There are two basic classes of commands used :
Lifecycle commands : commands used by OCS to control the lifecycle
characteristics of applications. Users generally do not need to be concerned with the lifecycle commands because they are implemented by the underlying infrastructure.
<P>
Functional commands : commands that implement the specific functional characteristics of a
subsystem components.
<P>
Functional operation is based on the Command/Action/Response model that isolates the transmission of the command from the resulting action that is performed. When an application receives a command, it validates any Configuration associated with that command and immediately accepts or rejects the command. If the command is accepted, the application then initiates an independent internal action to meet the conditions imposed by the command. Once those conditions have been met, an event is posted signifying the successful completion of the action (or the unsuccessful completion if the conditions can not be met).
<P>
Commands return immediately but the actions that are initiated as a result of a command may take some time to complete. When the action completes, an action status event is posted that includes the completion status of that action. The subsystem generating the command monitors this status event prior to issuing the command on the remote system. While the monitoring is performed automatically by the command system, Subsystem developers may need to attach a callback to perform processing on action completion. This callback may be null if no processing is needed.
<P>
If a command is accepted by the subsystem it causes an independent action to begin. A response to the command is returned immediately. The action begins matching the current configuration to the new demand configuration. When the configurations match (i.e., the subsystem has performed the input operations) the action signals the successful end of the action. If the configurations cannot be matched (whether by hardware failure, external stop command, timeout, or some other fault) the action signals the unsuccessful end of the action.
<P>
The important features of the command/action/response model are:
<P>
<UL>
<LI>Commands are never blocked. As soon as one command is started, another one can be issued. The
behavior of the controller when two or more configurations are started can be configured on a
per subsystem basis.
<LI>The actions are performed using one or more separate threads. They can be tuned for priority, number of
simultaneous actions, critical resources, or any other parameters.
<LI>Action completions produce events that tell the state of the current configuration. Actions push the lifecycle of the configuration through to completion.
<LI>Responses may be monitored by any other subsystems.<P>
</UL><P>


<H4>Generic subsystem control state commands</H4>
<UL>
<TABLE BORDER=3 CELLPADDING=5 BGCOLOR=LightBlue WIDTH=600>
<TR BGCOLOR=yellow><B><TD>Command</TD><TD>Description</TD></B></TR>
<TR><TD>start</TD><TD>Prepare the subsystem to accept functional commands </TD></TR>
<TR><TD>stop</TD><TD>Stop execution of a command</TD></TR>
<TR><TD>online</TD><TD>Set subsystem ready for commands</TD></TR>
<TR><TD>abort</TD><TD>Take subsystem offline (must be brought back online before any
other commanding is possible)</TD></TR>
</TABLE>
</UL><P>

<H3> 3.1.1 Interface Processing Time Requirements</H3>
Command messages issued via the middleware must be received by the computer system(s)
of the commanded subsystem within  2ms. A preliminary response (ACK) must be issued within
2ms and received by the caller within 5ms of the command origination time.
 
<H3> 3.1.2 Message Requirements</H3>

Every stream includes items for consistency<BR>
checking and performance monitoring support
<P><UL>
<TABLE BORDER=3 CELLPADDING=5 BGCOLOR=LightBlue WIDTH=600>
<TR BGCOLOR=yellow><B><TD>Identifier</TD><TD>Description</TD></B></TR>
<TR><TD>private_revCode</TD><TD>crc of IDL source</TD></TR>
<TR><TD>private_sndStamp</TD><TD> system time of sender</TD></TR>
<TR><TD>private_rcvStamp</TD><TD>system time of receiver</TD></TR>
<TR><TD>private_seqNum</TD><TD>sequence number (process)</TD></TR>
<TR><TD>private_origin</TD><TD>IP subaddr and PID</TD></TR>
</TABLE>
</UL>
<H3> 3.2 Telemetry Requirements</H3>
Telemetry data issued via the middleware must be received by the computer system(s)
of the Facility database , and any other subscribers , within 2ms. 

<H3> 3.3 Event Notifications Requirements</H3>
Any application may
post notifications and/or subscribe to notifications posted elsewhere. The notification service is robust and high performance. A notification consists of a topic and a severity. A sequence of notifications with the same topic is referred to as an event.
<P>
The topic is used to identify publishers to subscribers. The severity may be used as a filter
by notification subscribers.
<P>
The notification service has the following general properties:
An notification topic represents a many to many mapping: notifications may be posted to the topic from more than one source and received by zero or more targets. (Typically, however, most topics will have a single source.)
<P>
Notifications posted by a single source into an notification topic are received by all targets in the same order as they were posted.
<P>
Delivery of notifications to one subscriber cannot be blocked by the actions of another subscriber.
An notification stream is an abstract concept: a subscriber may subscribe to an notification stream using a wildcarded name in which case the notifications it receives are the merging of all published notifications whose names match that wildcarded name. 
<P>
Notification are not queued by the service. A late subscriber will not see earlier notifications.
<P>
The service does not drop notifications. A published notification will be delivered to all subscribers.
<P>
The notification service supports arbitrary notification topics.
<P>
Notifications are automatically tagged with the source and a timestamp.

<H3> 3.4 Communication Methods</H3>

<H3>3.4.1 Initiation : DDS discovery</H3>

The process by which domain participants find out about each others entities
Each participant maintains database on other participants in the domain and their entities
happens automatically behind the scenes (anonymous publish-subscribe) 
<UL>
<LI>Does not cross domain boundaries
<LI>Dynamic discovery
<LI>Participants must refresh their presence in the domain or will be aged out of database
<LI>QoS changes are propagated to remote participants
<LI>Two consecutive phases
<LI>Participant discovery phase
<LI>Participants discover each other
<LI>Best-effort communication
<LI>Endpoint discovery phase 
<LI>Participants exchange information about their datawriter and datareader entities
<LI>Reliable communication
<LI>Steady state traffic to maintain liveliness of participants
<LI>Participants periodically announce their presence using RTPS VAR message
<LI>Contains participant GUID, transport locators, QoS
<LI>Initially sent to all participants in initial peers list, then sent periodically to all discovered participants 
<LI>Sent using best-effort
</UL>

<P>
DataWriter/DataReader discovery 
<UL>
<LI>Send out pub/sub VAR to every new participant
<LI>NACK for pub/sub info if not received from a known participant  
<LI>Send out changes/additions/deletions to each participant
<LI>Uses reliable communication between participants
<LI>Data Distribution Service matches up local and remote entities to establish communication paths
</UL><P>

<IMG SRC="discovery.jpg">
<P>
Discovery is implemented using DDS entities known as Built-in Data Writers and Built-in Data Readers
<UL>
<LI>Uses same infrastructure as user defined Data Writers/Data Readers
<LI>Participant data is sent best effort 
<LI>Publication/subscription data is sent reliably
</UL>
<P>
Three Built-in topics (keyed):
<UL>
<LI>DCPSParticipant
<LI>DCPSPublication
<LI>DCPSSubscription
</UL>
Each participant on the same host and in the same domain requires a unique participant index
<P>
</UL></UL></UL>
<P>
<P>
<H3>3.4.2 Flow Control : DDS topics</H3>

Topics provide the basic connection point
between publishers and subscribers. The Topic of
a given publisher on one node must match the
Topic of an associated subscriber on any other
node. If the Topics do not match, communication
will not take place.
<P>
A Topic is comprised of a Topic Name and a
Topic Type. The Topic Name is a string that
uniquely identifies the Topic within a domain.
The Topic Type is the definition of the data
contained within the Topic. Topics must be
uniquely defined within any one particular
domain. Two Topics with different Topic Names
but the same Topic Type definition would be
considered two different Topics within the DDS
infrastructure.
<P>
<H3> 3.5 Security Requirements</H3>
<H3>3.5.1 Message timestamps</H3>

Message integrity is enhanced by the inclusion of egress-time and
arrival time (local system clocks) field in every topic
(command , notification, and telemetry). The SAL software
automatically performs validation to ensure early
detection of clock slew or other time related problems.

<H3>3.5.2 Software versioning checksums</H3>

Communications consistency and security is supported by the
inclusion of CRC checksum fields in every topic
definition (command , notification, and telemetry).
The SAL software automatically checks that the publisher and
subscribers are running code generated using identical 
(at the source code level) topic definitions. This prevents
problems associated with maintaining consistent inter-subsystem
interfaces across a widely distributed software development
infrastructure.



<H2>4.0 QUALIFICATION METHODS</H2>

<H3>4.1 System dictionary</H3>

A systemwide dictionary of all subsystems, devices, 
actions and states is maintained. All the interactions
between subsystems are automatically checked to verify
that only objects defined in the dictionary can be
used or referenced. See Appendix A.

<H3>4.2 Command definition database</H3>

A database of permissible commands is maintained
on a per subsystem basis. The database references the
system dictionary and contains 1 record per command.
Each command is constrained in terms of target
subsystems, parameter ranges, maximum frequency, 
timeout, pause/hold potential,and failure severity.
<P>
The database is used to automatically generate 
application level code to perform all command
level interactions. This code is thus guaranteed to be
consistent system wide. See Appendix C


<H3>4.3 Telemetry datastream Definition database</H3>

All telemetry datastream definitions are stored in a 
database. Each definition is automatically verified
for compliance with the system dictionary. Telemetry
items are detailed in terms of type, size, frequency, 
units, and value ranges. Any item with a physical 
correlate has an SI unit associated with it. 
<P>
The database is used to automatically generate 
application level code to perform all datastream
topic references. This code is thus guaranteed to be
consistent system wide. See Appendix B
<P>
Events are generated using the same database, and comprise
a special category of topics which may assigned the highest
priority (ALERT category) if appropriate.
<P>

<H3>4.4 Code generation</H3>
The primary implementation of the software interface described in this
document will be automatically generated. A software abstraction layer
(SAL) will provide a standardized wrapper for the low-level OMG DDS 
functionality which provides the transport layer. 
<P>
The permissible commands, datastream contents, and issuable alerts are
all defined by the controls system database and their nomenclature is controlled
by the system dictionary. All intersubsystem messages formats are
autogenerated. Low level data transfers include versioning checksums
based on the source level record definition.
<P>
<H3>4.5 Testing and simulation</H3>
Test servers and clients are generated which implement the full
set of commands, datastreams, and notifications are defined by
the controls system database. Tests may be configured for a 
variable number of servers/clients and automatically monitored
to ensure compliance with bandwidth and latency requirements.
All test results are archived to the facility database for 
future examination.

<H2>5.0       NOTES</H2>

<H2>6.0       APPENDICES</H2>


</BODY></HTML>


